{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "# Third-party library imports~\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch and related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# einops library for tensor operations\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "# Custom TINTO library imports\n",
    "from TINTOlib.tinto import TINTO\n",
    "from TINTOlib.supertml import SuperTML\n",
    "from TINTOlib.igtd import IGTD\n",
    "from TINTOlib.refined import REFINED\n",
    "from TINTOlib.barGraph import BarGraph\n",
    "from TINTOlib.distanceMatrix import DistanceMatrix\n",
    "from TINTOlib.combination import Combination\n",
    "from TINTOlib.featureWrap import FeatureWrap\n",
    "from TINTOlib.bie import BIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Version: 11.8\n",
      "cuDNN Version: 90100\n",
      "PyTorch Version: 2.5.1+cu118\n",
      "CUDA is available. PyTorch can use GPU.\n",
      "Current GPU: NVIDIA GeForce RTX 3080\n",
      "Is this tensor on GPU? True\n",
      "Is CUDA initialized? True\n",
      "Number of available GPUs: 1\n",
      "Current device index: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Get CUDA version\n",
    "cuda_version = torch.version.cuda\n",
    "print(f\"CUDA Version: {cuda_version}\")\n",
    "\n",
    "# Get cuDNN version\n",
    "cudnn_version = torch.backends.cudnn.version()\n",
    "print(f\"cuDNN Version: {cudnn_version}\")\n",
    "\n",
    "# Get PyTorch version\n",
    "pytorch_version = torch.__version__\n",
    "print(f\"PyTorch Version: {pytorch_version}\")\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. PyTorch can use GPU.\")\n",
    "    \n",
    "    # Get the name of the current GPU\n",
    "    print(f\"Current GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Create a random tensor and move it to GPU to verify\n",
    "    x = torch.rand(5, 3)\n",
    "    print(f\"Is this tensor on GPU? {x.cuda().is_cuda}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch will use CPU.\")\n",
    "\n",
    "# Additional check: is CUDA initialized?\n",
    "print(f\"Is CUDA initialized? {torch.cuda.is_initialized()}\")\n",
    "\n",
    "# Number of available GPUs\n",
    "print(f\"Number of available GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "# Current device index\n",
    "print(f\"Current device index: {torch.cuda.current_device()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 64\n",
    "# SET RANDOM SEED FOR REPRODUCIBILITY\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variable to store dataset name\n",
    "dataset_name = 'california_housing'\n",
    "results_path = f'logs/Regression/{dataset_name}/ViT+MLP_Regression'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"Datasets/Regression/{dataset_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the second-to-last column if MIMO\n",
    "# df = df.drop(df.columns[-2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedHouseVal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  MedHouseVal  \n",
       "0    -122.23        4.526  \n",
       "1    -122.22        3.585  \n",
       "2    -122.24        3.521  \n",
       "3    -122.25        3.413  \n",
       "4    -122.25        3.422  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LOAD AND PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=32):\n",
    "\n",
    "    # Generate the images if the folder does not exist\n",
    "    if not os.path.exists(images_folder):\n",
    "        #Generate thet images\n",
    "        image_model.generateImages(df, images_folder)\n",
    "    else:\n",
    "        print(\"The images are already generated\")\n",
    "\n",
    "    img_paths = os.path.join(images_folder,problem_type+\".csv\")\n",
    "\n",
    "    print(img_paths)\n",
    "    \n",
    "    imgs = pd.read_csv(img_paths)\n",
    "\n",
    "    # Update image paths\n",
    "    imgs[\"images\"] = images_folder + \"/\" + imgs[\"images\"]\n",
    "\n",
    "    # Combine datasets\n",
    "    combined_dataset = pd.concat([imgs, df], axis=1)\n",
    "\n",
    "    # Split data\n",
    "    df_x = combined_dataset.drop(df.columns[-1], axis=1).drop(\"values\", axis=1)\n",
    "    df_y = combined_dataset[\"values\"]\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(df_x, df_y, test_size=0.20, random_state=SEED)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.50, random_state=SEED)\n",
    "    # Numerical data\n",
    "    X_train_num = X_train.drop(\"images\", axis=1)\n",
    "    X_val_num = X_val.drop(\"images\", axis=1)\n",
    "    X_test_num = X_test.drop(\"images\", axis=1)\n",
    "\n",
    "    # Image data\n",
    "    X_train_img = np.array([cv2.imread(img) for img in X_train[\"images\"]])\n",
    "    X_val_img = np.array([cv2.imread(img) for img in X_val[\"images\"]])\n",
    "    X_test_img = np.array([cv2.imread(img) for img in X_test[\"images\"]])\n",
    "\n",
    "    # Create a MinMaxScaler object\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Scale numerical data\n",
    "    X_train_num = pd.DataFrame(scaler.fit_transform(X_train_num), columns=X_train_num.columns)\n",
    "    X_val_num = pd.DataFrame(scaler.transform(X_val_num), columns=X_val_num.columns)\n",
    "    X_test_num = pd.DataFrame(scaler.transform(X_test_num), columns=X_test_num.columns)\n",
    "\n",
    "    attributes = len(X_train_num.columns)\n",
    "    height, width, channels = X_train_img[0].shape\n",
    "    imgs_shape = (channels, height, width)\n",
    "\n",
    "    print(\"Images shape: \", imgs_shape)\n",
    "    print(\"Attributes: \", attributes)\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_num_tensor = torch.as_tensor(X_train_num.values, dtype=torch.float32)\n",
    "    X_val_num_tensor = torch.as_tensor(X_val_num.values, dtype=torch.float32)\n",
    "    X_test_num_tensor = torch.as_tensor(X_test_num.values, dtype=torch.float32)\n",
    "    X_train_img_tensor = torch.as_tensor(X_train_img, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "    X_val_img_tensor = torch.as_tensor(X_val_img, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "    X_test_img_tensor = torch.as_tensor(X_test_img, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "    y_train_tensor = torch.as_tensor(y_train.values, dtype=torch.float32).reshape(-1, 1)\n",
    "    y_val_tensor = torch.as_tensor(y_val.values, dtype=torch.float32).reshape(-1, 1)\n",
    "    y_test_tensor = torch.as_tensor(y_test.values, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset(X_train_num_tensor, X_train_img_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_num_tensor, X_val_img_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_num_tensor, X_test_img_tensor, y_test_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, attributes, imgs_shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MODEL ARCHITECTURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out), attn\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
    "                FeedForward(dim, mlp_dim, dropout = dropout)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        attentions = []  # Initialize a list to store attention weights\n",
    "        for attn, ff in self.layers:\n",
    "            attn_out, attn_weights = attn(x)  # Get both output and attention weights\n",
    "            x = attn_out + x  # Residual connection\n",
    "            attentions.append(attn_weights)  # Store attention weights\n",
    "            x = ff(x) + x  # Apply feedforward and residual connection\n",
    "\n",
    "        return self.norm(x), attentions\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x, attentions = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        return {\"last_hidden_state\": x, \"attentions\": attentions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_divisors(n):\n",
    "    divisors = []\n",
    "    for i in range(1, int(n**0.5) + 1):\n",
    "        if n % i == 0:\n",
    "            divisors.append(i)\n",
    "            if i != n // i:  # Check to include both divisors if they are not the same\n",
    "                divisors.append(n // i)\n",
    "    divisors.sort()\n",
    "    return divisors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate Fusion by concatenation\n",
    "# Process independently each modaility and combines their embeddings. \n",
    "class Model1(nn.Module):\n",
    "    def __init__(self, attributes, imgs_shape, patch_size):\n",
    "        super(Model1, self).__init__()\n",
    "        \n",
    "        # ViT branch\n",
    "        self.vit = ViT(\n",
    "            image_size = imgs_shape[1],\n",
    "            patch_size = patch_size,\n",
    "            dim = 128,\n",
    "            depth = 4,\n",
    "            heads = 8,\n",
    "            mlp_dim = 256\n",
    "        )\n",
    "\n",
    "        # MLP branch\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(attributes, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Final MLP\n",
    "        self.final_mlp = nn.Sequential(\n",
    "            nn.Linear(128+16, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, mlp_input, vit_input):\n",
    "        vit_output = self.vit(vit_input)\n",
    "        vit_hidden_state = vit_output[\"last_hidden_state\"]\n",
    "        attentions = vit_output[\"attentions\"]  # Extract the attention weights\n",
    "\n",
    "        mlp_output = self.mlp(mlp_input)\n",
    "        concat_output = torch.cat((mlp_output, vit_hidden_state), dim=1)\n",
    "        final_output = self.final_mlp(concat_output)\n",
    "        return final_output, attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate Fusion by gating mechanism (static gating)\n",
    "# Static Gating module that learns to emphasize or suppress each modality. \n",
    "# The gating weights are learnable parameters but remains the same for every sample.\n",
    "\n",
    "class StaticGating(nn.Module):\n",
    "    def __init__(self, num_features, img_features):\n",
    "        super(StaticGating, self).__init__()\n",
    "        \n",
    "        # Static learnable weights\n",
    "        self.num_gate = nn.Parameter(torch.tensor(0.5))\n",
    "        self.img_gate = nn.Parameter(torch.tensor(0.5))\n",
    "        \n",
    "        self.final_fc = nn.Linear(128, 128)\n",
    "\n",
    "    def forward(self, num_data, img_data):\n",
    "        \n",
    "        # Apply static gating\n",
    "        gated_output = self.num_gate * num_data + self.img_gate * img_data\n",
    "        return self.final_fc(gated_output)\n",
    "\n",
    "    \n",
    "class Model2(nn.Module):\n",
    "    def __init__(self, attributes, imgs_shape, patch_size):\n",
    "        super(Model2, self).__init__()\n",
    "        \n",
    "        # ViT branch\n",
    "        self.vit = ViT(\n",
    "            image_size = imgs_shape[1],\n",
    "            patch_size = patch_size,\n",
    "            dim = 128,\n",
    "            depth = 4,\n",
    "            heads = 8,\n",
    "            mlp_dim = 256\n",
    "        )\n",
    "\n",
    "        # MLP branch\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(attributes, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Linear layers to align both modalities to the same embedding dimension (128)\n",
    "        self.num_transform = nn.Linear(16, 128)  # Transform MLP output to 128-dim\n",
    "        self.img_transform = nn.Linear(128, 128)  # Transform ViT output to 128-dim\n",
    "        \n",
    "        # Gating Module\n",
    "        self.gating = StaticGating(num_features=16, img_features=128)\n",
    "        \n",
    "        # Final MLP\n",
    "        self.final_mlp = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, mlp_input, vit_input):\n",
    "        # Pass through MLP branch\n",
    "        mlp_output = self.mlp(mlp_input)\n",
    "        \n",
    "        # Pass through ViT branch\n",
    "        vit_output = self.vit(vit_input)\n",
    "        \n",
    "        # Transform both outputs to the same embedding dimension (128)\n",
    "        mlp_output = self.num_transform(mlp_output)  # Shape: (batch_size, 128)\n",
    "        vit_output = self.img_transform(vit_output)  # Shape: (batch_size, 128)\n",
    "        \n",
    "        # Apply gating mechanism\n",
    "        gated_output = self.gating(mlp_output, vit_output)\n",
    "        \n",
    "        # Pass through final MLP\n",
    "        return self.final_mlp(gated_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate Fusion by attention mechanism (dynamic gating)\n",
    "# Attention module that learns to emphasize or suppress each modality.\n",
    "# The attention weights changes based on input data. Limited to global weighting.\n",
    "\n",
    "class MultiheadAttentionGating(nn.Module):\n",
    "    def __init__(self, num_features, img_features):\n",
    "        super(MultiheadAttentionGating, self).__init__()\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(128 * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2),  # Two attention scores (one for each modality)\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, num_data, img_data):\n",
    "\n",
    "        # Concatenate transformed features\n",
    "        concat_data = torch.cat((num_data, img_data), dim=1)\n",
    "        \n",
    "        # Compute attention weights\n",
    "        attention_weights = self.attention(concat_data)  # Output: [batch_size, 2]\n",
    "        num_weight = attention_weights[:, 0].unsqueeze(1)  # Shape: [batch_size, 1]\n",
    "        img_weight = attention_weights[:, 1].unsqueeze(1)  # Shape: [batch_size, 1]\n",
    "        \n",
    "        # Weighted sum of modalities\n",
    "        return num_weight * num_data + img_weight * img_data\n",
    "\n",
    "\n",
    "\n",
    "class Model3(nn.Module):\n",
    "    def __init__(self, attributes, imgs_shape, patch_size):\n",
    "        super(Model3, self).__init__()\n",
    "        \n",
    "        # ViT branch\n",
    "        self.vit = ViT(\n",
    "            image_size = imgs_shape[1],\n",
    "            patch_size = patch_size,\n",
    "            dim = 128,\n",
    "            depth = 4,\n",
    "            heads = 8,\n",
    "            mlp_dim = 256\n",
    "        )\n",
    "\n",
    "        # MLP branch\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(attributes, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Linear layers to align both modalities to the same embedding dimension (128)\n",
    "        self.num_transform = nn.Linear(16, 128)  # Transform MLP output to 128-dim\n",
    "        self.img_transform = nn.Linear(128, 128)  # Transform ViT output to 128-dim\n",
    "        \n",
    "        # Gating mechanism for fusion\n",
    "        self.gating = MultiheadAttentionGating(num_features=16, img_features=128)\n",
    "        \n",
    "        # Final MLP\n",
    "        self.final_mlp = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, mlp_input, vit_input):\n",
    "        vit_output = self.vit(vit_input)\n",
    "        mlp_output = self.mlp(mlp_input)\n",
    "\n",
    "        # Transform both outputs to the same embedding dimension (128)\n",
    "        mlp_output = self.num_transform(mlp_output)  # Shape: (batch_size, 128)\n",
    "        vit_output = self.img_transform(vit_output)  # Shape: (batch_size, 128)\n",
    "        \n",
    "        # Apply gating mechanism to fuse outputs\n",
    "        fused_output = self.gating(mlp_output, vit_output)  # Output: [batch_size, 128]\n",
    "\n",
    "        # Final prediction through MLP\n",
    "        return self.final_mlp(fused_output)  # Output: [batch_size, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate Fusion by attention mechanism using Multihead(dynamic gating)\n",
    "# Attention module that learns to emphasize or suppress each modality.\n",
    "# Instead of commputing the weight based on a concatenation of the modalities, we compute the pairwise interaction between the two modalities.\n",
    "# The attention weights changes based on input data.\n",
    "# Use mean pooling, averages embeddings after attention. Simple tasks with balanced modality contributions.\n",
    "\n",
    "class MultiheadAttentionGatingWithMean(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiheadAttentionGatingWithMean, self).__init__()\n",
    "        \n",
    "        # Multihead attention layer\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)\n",
    "        \n",
    "        # Output linear layer for refining the fused representation\n",
    "        self.output_layer = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, num_data, img_data):\n",
    "        # Add a sequence dimension to the MLP and ViT outputs\n",
    "        num_data = num_data.unsqueeze(1)  # Shape: (batch_size, seq_len=1, embed_dim)\n",
    "        img_data = img_data.unsqueeze(1)  # Shape: (batch_size, seq_len=1, embed_dim)\n",
    "\n",
    "        # Concatenate the numeric and image data along the sequence dimension\n",
    "        combined_data = torch.cat((num_data, img_data), dim=1)  # Shape: (batch_size, seq_len=2, embed_dim)\n",
    "\n",
    "        # Apply multihead attention to the combined data\n",
    "        attn_output, _ = self.multihead_attn(query=combined_data, key=combined_data, value=combined_data)\n",
    "\n",
    "        # Perform mean pooling to reduce the sequence dimension to a single fused representation\n",
    "        fused_output = attn_output.mean(dim=1)  # Shape: (batch_size, embed_dim)\n",
    "\n",
    "        # Refine the fused representation through a linear layer\n",
    "        return self.output_layer(fused_output)\n",
    "\n",
    "class Model4(nn.Module):\n",
    "    def __init__(self, attributes, imgs_shape, patch_size):\n",
    "        super(Model4, self).__init__()\n",
    "        \n",
    "        # Vision Transformer (ViT) branch for processing image data\n",
    "        self.vit = ViT(\n",
    "            image_size=imgs_shape[1],  # Input image size\n",
    "            patch_size=patch_size,     # Patch size for ViT\n",
    "            dim=128,                   # Embedding dimension\n",
    "            depth=4,                   # Number of transformer layers\n",
    "            heads=8,                   # Number of attention heads\n",
    "            mlp_dim=256                # Hidden layer dimension in ViT\n",
    "        )\n",
    "\n",
    "        # MLP branch for processing tabular data\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(attributes, 16),  # First layer to project attributes\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),          # Second layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),          # Third layer to reduce dimensionality\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Linear layers to align both modalities to the same embedding dimension (128)\n",
    "        self.num_transform = nn.Linear(16, 128)  # Transform MLP output to 128-dim\n",
    "        self.img_transform = nn.Linear(128, 128)  # Transform ViT output to 128-dim\n",
    "        \n",
    "        # Multihead attention module for intermediate fusion\n",
    "        self.attention_fusion = MultiheadAttentionGatingWithMean(embed_dim=128, num_heads=4)\n",
    "\n",
    "        # Final MLP for prediction\n",
    "        self.final_mlp = nn.Sequential(\n",
    "            nn.Linear(128, 64),  # First layer to reduce dimensionality\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),   # Second layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),   # Third layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)     # Final layer for output\n",
    "        )\n",
    "\n",
    "    def forward(self, mlp_input, vit_input):\n",
    "        # Pass the tabular data through the MLP branch\n",
    "        mlp_output = self.mlp(mlp_input)  # Output shape: (batch_size, 16)\n",
    "        \n",
    "        # Pass the image data through the ViT branch\n",
    "        vit_output = self.vit(vit_input)  # Output shape: (batch_size, 128)\n",
    "\n",
    "        # Transform both outputs to the same embedding dimension (128)\n",
    "        mlp_output = self.num_transform(mlp_output)  # Shape: (batch_size, 128)\n",
    "        vit_output = self.img_transform(vit_output)  # Shape: (batch_size, 128)\n",
    "\n",
    "        # Fuse the outputs using multihead attention with mean pooling\n",
    "        fused_output = self.attention_fusion(mlp_output, vit_output)  # Shape: (batch_size, 128)\n",
    "\n",
    "        # Pass the fused representation through the final MLP for prediction\n",
    "        return self.final_mlp(fused_output)  # Final output shape: (batch_size, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate Fusion by attention mechanism using Multihead(dynamic gating)\n",
    "# Attention module that learns to emphasize or suppress each modality.\n",
    "# Instead of commputing the weight based on a concatenation of the modalities, we compute the pairwise interaction between the two modalities.\n",
    "# The attention weights changes based on input data.\n",
    "# Use CLS, uses a learnable token to summarize fusion. Tasks requiring adaptive fusion across modalities.\n",
    "\n",
    "class AttentionGatingWithCLS(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(AttentionGatingWithCLS, self).__init__()\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))  # Learnable CLS token\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.output_layer = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, num_data, img_data):\n",
    "        \n",
    "        # Add sequence dimension to MLP and ViT outputs\n",
    "        num_data = num_data.unsqueeze(1)  # Shape: (batch_size, 1, embed_dim)\n",
    "        img_data = img_data.unsqueeze(1)  # Shape: (batch_size, 1, embed_dim)\n",
    "\n",
    "        # Prepend CLS token\n",
    "        cls_token = self.cls_token.expand(num_data.size(0), -1, -1)  # (batch_size, 1, embed_dim)\n",
    "        combined_data = torch.cat((cls_token, num_data, img_data), dim=1)  # (batch_size, seq_len=3, embed_dim)\n",
    "\n",
    "        # Apply multi-head attention\n",
    "        attn_output, _ = self.multihead_attn(query=combined_data, key=combined_data, value=combined_data)\n",
    "\n",
    "        # Extract CLS token representation\n",
    "        cls_output = attn_output[:, 0, :]  # (batch_size, embed_dim)\n",
    "        return self.output_layer(cls_output)\n",
    "\n",
    "\n",
    "class Model5(nn.Module):\n",
    "    def __init__(self, attributes, imgs_shape, patch_size):\n",
    "        super(Model5, self).__init__()\n",
    "        \n",
    "        # Vision Transformer (ViT) branch for processing image data\n",
    "        self.vit = ViT(\n",
    "            image_size=imgs_shape[1],  # Input image size\n",
    "            patch_size=patch_size,     # Patch size for ViT\n",
    "            dim=128,                   # Embedding dimension\n",
    "            depth=4,                   # Number of transformer layers\n",
    "            heads=8,                   # Number of attention heads\n",
    "            mlp_dim=256                # Hidden layer dimension in ViT\n",
    "        )\n",
    "\n",
    "        # MLP branch for processing tabular data\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(attributes, 16),  # First layer to project attributes\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),          # Second layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),          # Third layer to reduce dimensionality\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Linear layers to align both modalities to the same embedding dimension (128)\n",
    "        self.num_transform = nn.Linear(16, 128)  # Transform MLP output to 128-dim\n",
    "        self.img_transform = nn.Linear(128, 128)  # Transform ViT output to 128-dim\n",
    "        \n",
    "        # Attention mechanism for fusion\n",
    "        self.attention_fusion = AttentionGatingWithCLS(embed_dim=128, num_heads=4)\n",
    "\n",
    "        # Final MLP\n",
    "        self.final_mlp = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, mlp_input, vit_input):\n",
    "        # Process individual branches\n",
    "        vit_output = self.vit(vit_input)  # (batch_size, embed_dim=128)\n",
    "        mlp_output = self.mlp(mlp_input)  # (batch_size, embed_dim=128)\n",
    "        \n",
    "        # Transform both outputs to the same embedding dimension (128)\n",
    "        mlp_output = self.num_transform(mlp_output)  # Shape: (batch_size, 128)\n",
    "        vit_output = self.img_transform(vit_output)  # Shape: (batch_size, 128)\n",
    "        \n",
    "        # Fuse outputs using attention with CLS token\n",
    "        fused_output = self.attention_fusion(mlp_output, vit_output)\n",
    "\n",
    "        # Final prediction\n",
    "        return self.final_mlp(fused_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate Fusion by attention mechanism using Multihead(dynamic gating)\n",
    "# CrossAttention module that learns to emphasize or suppress each modality.\n",
    "# Instead of commputing the pairwise interaction between the two modalities. We choose one modality as Query and \n",
    "# allow it to selectively focus on the most relevant features of the other modality, which serves as the Key and Value. \n",
    "# Tasks where one modality provides context or guides another modality.\n",
    "# The attention weights changes based on input data.\n",
    "# Use single modality representation.\n",
    "\n",
    "class CrossAttentionGating(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(CrossAttentionGating, self).__init__()\n",
    "\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.output_layer = nn.Linear(embed_dim, embed_dim)  # Linear projection after attention\n",
    "\n",
    "    def forward(self, num_data, img_data):\n",
    "  \n",
    "        # Add sequence dimension to MLP and ViT outputs\n",
    "        num_data = num_data.unsqueeze(1)  # Shape: (batch_size, 1, embed_dim)\n",
    "        img_data = img_data.unsqueeze(1)  # Shape: (batch_size, 1, embed_dim)\n",
    "\n",
    "        # Apply cross-attention: num_data (query) attends to img_data (key, value)\n",
    "        attn_output, _ = self.cross_attn(query=num_data, key=img_data, value=img_data)\n",
    "\n",
    "        # Remove sequence dimension from the attention output\n",
    "        fused_output = attn_output.squeeze(1)  # (batch_size, embed_dim)\n",
    "\n",
    "        # Pass through a linear layer to refine the representation\n",
    "        return self.output_layer(fused_output)\n",
    "\n",
    "\n",
    "class Model6(nn.Module):\n",
    "    def __init__(self, attributes, imgs_shape, patch_size):\n",
    "        super(Model6, self).__init__()\n",
    "        \n",
    "        # Vision Transformer (ViT) branch for processing image data\n",
    "        self.vit = ViT(\n",
    "            image_size=imgs_shape[1],  # Input image size\n",
    "            patch_size=patch_size,     # Patch size for ViT\n",
    "            dim=128,                   # Embedding dimension\n",
    "            depth=4,                   # Number of transformer layers\n",
    "            heads=8,                   # Number of attention heads\n",
    "            mlp_dim=256                # Hidden layer dimension in ViT\n",
    "        )\n",
    "\n",
    "        # MLP branch for processing tabular data\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(attributes, 16),  # First layer to project attributes\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),          # Second layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),          # Third layer to reduce dimensionality\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Linear layers to align both modalities to the same embedding dimension (128)\n",
    "        self.num_transform = nn.Linear(16, 128)  # Transform MLP output to 128-dim\n",
    "        self.img_transform = nn.Linear(128, 128)  # Transform ViT output to 128-dim     \n",
    "        \n",
    "        # Attention mechanism for fusion\n",
    "        self.attention_fusion = AttentionGatingWithCLS(embed_dim=128, num_heads=4)\n",
    "\n",
    "        # Final MLP\n",
    "        self.final_mlp = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, mlp_input, vit_input):\n",
    "        # Process individual branches\n",
    "        vit_output = self.vit(vit_input)  # (batch_size, embed_dim=128)\n",
    "        mlp_output = self.mlp(mlp_input)  # (batch_size, embed_dim=128)\n",
    "        \n",
    "        # Transform both outputs to the same embedding dimension (128)\n",
    "        mlp_output = self.num_transform(mlp_output)  # Shape: (batch_size, 128)\n",
    "        vit_output = self.img_transform(vit_output)  # Shape: (batch_size, 128)\n",
    "        \n",
    "        # Fuse outputs using attention with CLS token\n",
    "        fused_output = self.attention_fusion(mlp_output, vit_output)\n",
    "\n",
    "        # Final prediction\n",
    "        return self.final_mlp(fused_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intermediate Fusion by attention mechanism using Multihead(dynamic gating)\n",
    "# CrossAttention module that learns to emphasize or suppress each modality.\n",
    "# Instead of commputing the pairwise interaction between the two modalities. We choose one modality as Query and \n",
    "# allow it to selectively focus on the most relevant features of the other modality, which serves as the Key and Value.\n",
    "# The attention weights changes based on input data.\n",
    "# Use CLS, uses a learnable token to summarize fusion.\n",
    "\n",
    "class CrossAttentionGatingWithCLS(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(CrossAttentionGatingWithCLS, self).__init__()\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))  # Learnable CLS token\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.output_layer = nn.Linear(embed_dim, embed_dim)  # Linear projection after attention\n",
    "\n",
    "    def forward(self, num_data, img_data):\n",
    "        \"\"\"\n",
    "        num_data: Numerical modality (e.g., MLP output). Acts as Query (Q).\n",
    "        img_data: Image modality (e.g., ViT output). Acts as Key (K) and Value (V).\n",
    "        CLS token interacts with Key (K) and Value (V) to form a fused representation.\n",
    "        \"\"\"\n",
    "\n",
    "        # Add sequence dimension to numerical modality (Query)\n",
    "        num_data = num_data.unsqueeze(1)  # Shape: (batch_size, 1, embed_dim)\n",
    "\n",
    "        # Prepend CLS token to the numerical data (Query)\n",
    "        cls_token = self.cls_token.expand(num_data.size(0), -1, -1)  # (batch_size, 1, embed_dim)\n",
    "        query_data = torch.cat((cls_token, num_data), dim=1)  # (batch_size, seq_len=2, embed_dim)\n",
    "\n",
    "        # Add sequence dimension to image modality (Key, Value)\n",
    "        img_data = img_data.unsqueeze(1)  # Shape: (batch_size, 1, embed_dim)\n",
    "\n",
    "        # Apply cross-attention: Query interacts with Key and Value\n",
    "        attn_output, _ = self.cross_attn(query=query_data, key=img_data, value=img_data)\n",
    "\n",
    "        # Extract CLS token representation from attention output\n",
    "        fused_output = attn_output[:, 0, :]  # CLS token output: (batch_size, embed_dim)\n",
    "\n",
    "        # Refine the fused representation\n",
    "        return self.output_layer(fused_output)\n",
    "\n",
    "\n",
    "class Model7(nn.Module):\n",
    "    def __init__(self, attributes, imgs_shape, patch_size):\n",
    "        super(Model7, self).__init__()\n",
    "        \n",
    "        # Vision Transformer (ViT) branch for processing image data\n",
    "        self.vit = ViT(\n",
    "            image_size=imgs_shape[1],  # Input image size\n",
    "            patch_size=patch_size,     # Patch size for ViT\n",
    "            dim=128,                   # Embedding dimension\n",
    "            depth=4,                   # Number of transformer layers\n",
    "            heads=8,                   # Number of attention heads\n",
    "            mlp_dim=256                # Hidden layer dimension in ViT\n",
    "        )\n",
    "\n",
    "        # MLP branch for processing tabular data\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(attributes, 16),  # First layer to project attributes\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 32),          # Second layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),          # Third layer to reduce dimensionality\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Linear layers to align both modalities to the same embedding dimension (128)\n",
    "        self.num_transform = nn.Linear(16, 128)  # Transform MLP output to 128-dim\n",
    "        self.img_transform = nn.Linear(128, 128)  # Transform ViT output to 128-dim\n",
    "        \n",
    "        # Cross-attention mechanism with CLS token\n",
    "        self.cross_attention_fusion = CrossAttentionGatingWithCLS(embed_dim=128, num_heads=4)\n",
    "\n",
    "        # Final MLP\n",
    "        self.final_mlp = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, mlp_input, vit_input):\n",
    "        # Process individual branches\n",
    "        vit_output = self.vit(vit_input)  # (batch_size, embed_dim=128)\n",
    "        mlp_output = self.mlp(mlp_input)  # (batch_size, embed_dim=128)\n",
    "\n",
    "        # Transform both outputs to the same embedding dimension (128)\n",
    "        mlp_output = self.num_transform(mlp_output)  # Shape: (batch_size, 128)\n",
    "        vit_output = self.img_transform(vit_output)  # Shape: (batch_size, 128)\n",
    "        \n",
    "        # Fuse outputs using cross-attention with CLS token\n",
    "        fused_output = self.cross_attention_fusion(mlp_output, vit_output)\n",
    "\n",
    "        # Final prediction\n",
    "        return self.final_mlp(fused_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = TINTO(problem= problem_type, blur=True, random_seed=SEED)\n",
    "#image_model = REFINED(problem= problem_type,hcIterations=5)\n",
    "#image_model = IGTD(problem= problem_type)\n",
    "#image_model = BarGraph(problem= problem_type)\n",
    "#image_model = DistanceMatrix(problem= problem_type)\n",
    "#image_model = Combination(problem= problem_type)\n",
    "#image_model = SuperTML(problem= problem_type)\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_TINTO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iterations_per_epoch(dataset_size, batch_size):\n",
    "    iterations = dataset_size // batch_size\n",
    "    if dataset_size % batch_size != 0:\n",
    "        iterations += 1\n",
    "    return iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = calculate_iterations_per_epoch(df.shape[0], batch_size)\n",
    "# For the Boston dataset, the number of samples is too small for a range test, so the number of epochs is tripled.\n",
    "#num_epochs = num_epochs*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "645"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EXPERIMENT 2: IGTD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Get the shape of the dataframe\n",
    "num_columns = df.shape[1]\n",
    "\n",
    "# Calculate number of columns - 1\n",
    "columns_minus_one = num_columns - 1\n",
    "\n",
    "# Calculate the square root for image size\n",
    "import math\n",
    "image_size = math.ceil(math.sqrt(columns_minus_one))\n",
    "print(image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = IGTD(problem= problem_type, scale=[image_size,image_size], fea_dist_method='Euclidean', image_dist_method='Euclidean', error='abs', max_step=30000, val_step=300, random_seed=SEED)\n",
    "name = f\"IGTD_{image_size}x{image_size}_fEuclidean_iEuclidean_abs\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The images are already generated\n",
      "HyNNImages/Regression/california_housing/images_california_housing_IGTD_3x3_fEuclidean_iEuclidean_abs\\regression.csv\n",
      "Images shape:  (3, 3, 3)\n",
      "Attributes:  8\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"IGTD_3x3_fEuclidean_iEuclidean_abs_Model1_concat_patch_s1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model_path = f\"models/Regression/{dataset_name}/ViT+MLP/{model_name}/best_model.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model1(attributes, imgs_shape, patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jiayu\\AppData\\Local\\Temp\\ipykernel_27188\\1481618732.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model1(\n",
       "  (vit): ViT(\n",
       "    (to_patch_embedding): Sequential(\n",
       "      (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=1, p2=1)\n",
       "      (1): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): Linear(in_features=3, out_features=128, bias=True)\n",
       "      (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (transformer): Transformer(\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x ModuleList(\n",
       "          (0): Attention(\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (attend): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_qkv): Linear(in_features=128, out_features=1536, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=256, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Dropout(p=0.0, inplace=False)\n",
       "              (4): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (5): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (to_latent): Identity()\n",
       "  )\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=8, out_features=16, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (5): ReLU()\n",
       "  )\n",
       "  (final_mlp): Sequential(\n",
       "    (0): Linear(in_features=144, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=16, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tabular data: tensor([0.1360, 0.2941, 0.0291, 0.0210, 0.0438, 0.0024, 0.6302, 0.1673])\n"
     ]
    }
   ],
   "source": [
    "# Assume `test_loader` is a PyTorch DataLoader\n",
    "# Fetch one batch of images and labels\n",
    "data_iter = iter(test_loader)\n",
    "num_data, images, labels = next(data_iter)  # Get one batch (images and labels)\n",
    "\n",
    "# Select the first tabular data, image and its corresponding label (optional)\n",
    "num = num_data[0]\n",
    "image = images[0]  # Shape: (C, H, W)\n",
    "label = labels[0]  # (Optional, if labels are available)\n",
    "\n",
    "# Visualization (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "# Convert the tensor to a PIL image for visualization\n",
    "plt.imshow(F.to_pil_image(image))\n",
    "plt.title(f\"Label: {label.item() if labels is not None else 'No label available'}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"tabular data: {num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape for model input: torch.Size([1, 3, 3, 3])\n",
      "Tabular data shape for model input: torch.Size([1, 8])\n"
     ]
    }
   ],
   "source": [
    "# Ensure the image has the correct shape for the model\n",
    "image = image.unsqueeze(0).to(device)  # Add batch dimension and move to device (e.g., GPU)\n",
    "num = num.unsqueeze(0).to(device)\n",
    "print(\"Image shape for model input:\", image.shape)\n",
    "print(\"Tabular data shape for model input:\", num.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize the attention map\n",
    "def normalize_attention_map(attention_map):\n",
    "    # Apply ReLU to remove negative values\n",
    "    attention_map = np.maximum(attention_map, 0)\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    attention_map -= np.min(attention_map)\n",
    "    attention_map /= np.max(attention_map)  # Ensure max is 1\n",
    "    return attention_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multimodal Fusion values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass through the model\n",
    "with torch.no_grad():\n",
    "    outputs, attentions = model(num, image)\n",
    "\n",
    "model.gating.num_gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAW Attention Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By extracting the attention weights from each attention head at various layers, you can visualize heatmaps over the input image patches. These maps show which regions of the image a particular head or layer attends to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [94.0..255.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patches (excluding CLS token): 9, Patch size: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#1 RAW Attention Map\n",
    "from PIL import Image  # Import Image module\n",
    "\n",
    "# Forward pass through the model\n",
    "with torch.no_grad():\n",
    "    outputs, attentions = model(num, image)\n",
    "\n",
    "# Function to visualize attention map with color bar\n",
    "def visualize_attention(image, attention_map, title=\"Attention Map\"):\n",
    "    # Ensure the image is in the correct format (C, H, W -> H, W, C)\n",
    "    if image.ndimension() == 4:  # If the input image has batch size\n",
    "        image = image[0]  # Use the first image in the batch\n",
    "    image = image.permute(1, 2, 0).cpu().numpy()  # Convert to HWC format (Height, Width, Channels)\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    \n",
    "    # Display the image\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    # Overlay the attention map\n",
    "    im = plt.imshow(attention_map, cmap=\"jet\", alpha=0.5)  # Overlay attention with transparency\n",
    "    \n",
    "    # Add title and hide axes\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # Add color bar for the attention map\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)  # Show the color bar for the attention map\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# 1. Raw Attention Map\n",
    "# Extract attention weights from the last layer, average over all heads\n",
    "last_layer_attention = attentions[-1]  # Shape: (batch_size, num_heads, num_patches, num_patches)\n",
    "avg_attention = last_layer_attention.mean(dim=1)  # Average over heads\n",
    "\n",
    "# Get the shape of avg_attention to determine the number of patches\n",
    "num_patches = avg_attention.shape[-1]  # Number of patches (including CLS token)\n",
    "\n",
    "# Calculate the size of the attention map based on number of patches\n",
    "patch_size = int(np.sqrt(num_patches - 1))  # Exclude CLS token\n",
    "print(f\"Number of patches (excluding CLS token): {num_patches - 1}, Patch size: {patch_size}\")\n",
    "\n",
    "# Extract attention from CLS token to all patches and reshape to a grid\n",
    "cls_attention = avg_attention[0, 0, 1:].reshape(patch_size, patch_size).detach().cpu().numpy()  # Exclude CLS token itself\n",
    "\n",
    "# Normalize the attention map\n",
    "cls_attention_normalized = normalize_attention_map(cls_attention)\n",
    "\n",
    "# Resize attention map to match image size\n",
    "cls_attention_resized = np.array(Image.fromarray(cls_attention_normalized).resize((imgs_shape[1], imgs_shape[1]), resample=Image.BILINEAR))\n",
    "\n",
    "# Visualize Raw Attention Map\n",
    "visualize_attention(image, cls_attention_resized, title=\"Raw Attention Map (Last Layer)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since attention is compositional through layers, researchers use attention rollout techniques to aggregate attention across layers. For instance, one can multiply attention matrices across layers to track how information flows from patches to the CLS token. This can yield a single interpretable map that highlights the most influential patches for the final decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [94.0..255.0].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Attention Rollout\n",
    "# sxplain brieflyy\n",
    "def attention_rollout(attentions):\n",
    "    num_tokens = attentions[0].size(-1)\n",
    "    eye = torch.eye(num_tokens).unsqueeze(0).to(attentions[0].device)  # Identity matrix for residual connection\n",
    "    joint_attention = eye\n",
    "    for attention in attentions:\n",
    "        avg_attention = attention.mean(dim=1)  # Average over heads\n",
    "        joint_attention = torch.matmul(joint_attention, avg_attention + eye)  # Residual connection\n",
    "    return joint_attention\n",
    "\n",
    "rollout_attention = attention_rollout(attentions)\n",
    "cls_rollout_attention = rollout_attention[0, 0, 1:].reshape(patch_size, patch_size).detach().cpu().numpy()\n",
    "\n",
    "# Normalize the rollout attention map\n",
    "cls_rollout_attention_normalized = normalize_attention_map(cls_rollout_attention)\n",
    "\n",
    "# Resize rollout attention map to match image size\n",
    "rollout_attention_resized = np.array(Image.fromarray(cls_rollout_attention_normalized).resize((imgs_shape[1], imgs_shape[1]), resample=Image.BILINEAR))\n",
    "\n",
    "# Visualize Attention Rollout Map\n",
    "visualize_attention(image, rollout_attention_resized, title=\"Attention Rollout Map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient based - Grad-Cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure input tensor supports gradients\n",
    "image.requires_grad_()\n",
    "\n",
    "# Forward pass to get output and attentions\n",
    "outputs, attentions = model(image)\n",
    "\n",
    "# Assuming regression task, extract scalar output\n",
    "output_scalar = outputs.squeeze()  # Single scalar value for batch size = 1\n",
    "regression_value = output_scalar.item()\n",
    "\n",
    "# Extract attention map from the last layer\n",
    "last_attention_map = attentions[-1]  # Shape: (Batch, Heads, Tokens, Tokens)\n",
    "\n",
    "# We need gradients w.r.t. this attention map\n",
    "last_attention_map.requires_grad_()\n",
    "\n",
    "# Zero gradients in the model\n",
    "model.zero_grad()\n",
    "\n",
    "# Backward pass to compute gradients of the regression output\n",
    "output_scalar.backward()\n",
    "\n",
    "# Now, last_attention_map.grad contains the gradients\n",
    "gradients = last_attention_map.grad  # Gradients w.r.t. attention map\n",
    "\n",
    "# Normalize and visualize the gradients as a heatmap\n",
    "# Use the mean of the gradients across the heads\n",
    "pooled_gradients = torch.mean(gradients, dim=1)  # Mean over heads\n",
    "attention_gradient_map = pooled_gradients.squeeze()  # Shape: (Tokens, Tokens)\n",
    "\n",
    "# Reshape or resize the gradient map if needed for visualization\n",
    "# ViT token attention maps may need to be converted back to image space\n",
    "def normalize_attention_map(attention_map):\n",
    "    attention_map -= attention_map.min()\n",
    "    attention_map /= attention_map.max()\n",
    "    return attention_map\n",
    "\n",
    "# Normalize and visualize\n",
    "attention_gradient_map_normalized = normalize_attention_map(attention_gradient_map.cpu().detach().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
