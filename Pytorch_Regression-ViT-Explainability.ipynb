{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "# Third-party library imports~\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch and related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# einops library for tensor operations\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "# Custom TINTO library imports\n",
    "from TINTOlib.tinto import TINTO\n",
    "from TINTOlib.supertml import SuperTML\n",
    "from TINTOlib.igtd import IGTD\n",
    "from TINTOlib.refined import REFINED\n",
    "from TINTOlib.barGraph import BarGraph\n",
    "from TINTOlib.distanceMatrix import DistanceMatrix\n",
    "from TINTOlib.combination import Combination\n",
    "from TINTOlib.featureWrap import FeatureWrap\n",
    "from TINTOlib.bie import BIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Version: 11.8\n",
      "cuDNN Version: 90100\n",
      "PyTorch Version: 2.5.1+cu118\n",
      "CUDA is available. PyTorch can use GPU.\n",
      "Current GPU: NVIDIA GeForce RTX 3080\n",
      "Is this tensor on GPU? True\n",
      "Is CUDA initialized? True\n",
      "Number of available GPUs: 1\n",
      "Current device index: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Get CUDA version\n",
    "cuda_version = torch.version.cuda\n",
    "print(f\"CUDA Version: {cuda_version}\")\n",
    "\n",
    "# Get cuDNN version\n",
    "cudnn_version = torch.backends.cudnn.version()\n",
    "print(f\"cuDNN Version: {cudnn_version}\")\n",
    "\n",
    "# Get PyTorch version\n",
    "pytorch_version = torch.__version__\n",
    "print(f\"PyTorch Version: {pytorch_version}\")\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. PyTorch can use GPU.\")\n",
    "    \n",
    "    # Get the name of the current GPU\n",
    "    print(f\"Current GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Create a random tensor and move it to GPU to verify\n",
    "    x = torch.rand(5, 3)\n",
    "    print(f\"Is this tensor on GPU? {x.cuda().is_cuda}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch will use CPU.\")\n",
    "\n",
    "# Additional check: is CUDA initialized?\n",
    "print(f\"Is CUDA initialized? {torch.cuda.is_initialized()}\")\n",
    "\n",
    "# Number of available GPUs\n",
    "print(f\"Number of available GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "# Current device index\n",
    "print(f\"Current device index: {torch.cuda.current_device()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 64\n",
    "# SET RANDOM SEED FOR REPRODUCIBILITY\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variable to store dataset name\n",
    "dataset_name = 'california_housing'\n",
    "results_path = f'logs/Regression/{dataset_name}/ViT_Regression'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"Datasets/Regression/{dataset_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20640, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the second-to-last column if MIMO\n",
    "# df = df.drop(df.columns[-2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedHouseVal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  MedHouseVal  \n",
       "0    -122.23        4.526  \n",
       "1    -122.22        3.585  \n",
       "2    -122.24        3.521  \n",
       "3    -122.25        3.413  \n",
       "4    -122.25        3.422  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## LOAD AND PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=32):\n",
    "\n",
    "    # Generate the images if the folder does not exist\n",
    "    if not os.path.exists(images_folder):\n",
    "        #Generate thet images\n",
    "        image_model.generateImages(df, images_folder)\n",
    "    else:\n",
    "        print(\"The images are already generated\")\n",
    "\n",
    "    img_paths = os.path.join(images_folder,problem_type+\".csv\")\n",
    "\n",
    "    print(img_paths)\n",
    "    \n",
    "    imgs = pd.read_csv(img_paths)\n",
    "\n",
    "    # Update image paths\n",
    "    imgs[\"images\"] = images_folder + \"/\" + imgs[\"images\"]\n",
    "\n",
    "    # Combine datasets\n",
    "    combined_dataset = pd.concat([imgs, df], axis=1)\n",
    "\n",
    "    # Split data\n",
    "    df_x = combined_dataset.drop(df.columns[-1], axis=1).drop(\"values\", axis=1)\n",
    "    df_y = combined_dataset[\"values\"]\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(df_x, df_y, test_size=0.20, random_state=SEED)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.50, random_state=SEED)\n",
    "    # Numerical data\n",
    "    X_train_num = X_train.drop(\"images\", axis=1)\n",
    "    X_val_num = X_val.drop(\"images\", axis=1)\n",
    "    X_test_num = X_test.drop(\"images\", axis=1)\n",
    "\n",
    "    # Image data\n",
    "    X_train_img = np.array([cv2.imread(img) for img in X_train[\"images\"]])\n",
    "    X_val_img = np.array([cv2.imread(img) for img in X_val[\"images\"]])\n",
    "    X_test_img = np.array([cv2.imread(img) for img in X_test[\"images\"]])\n",
    "\n",
    "    # Create a MinMaxScaler object\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Scale numerical data\n",
    "    X_train_num = pd.DataFrame(scaler.fit_transform(X_train_num), columns=X_train_num.columns)\n",
    "    X_val_num = pd.DataFrame(scaler.transform(X_val_num), columns=X_val_num.columns)\n",
    "    X_test_num = pd.DataFrame(scaler.transform(X_test_num), columns=X_test_num.columns)\n",
    "\n",
    "    attributes = len(X_train_num.columns)\n",
    "    height, width, channels = X_train_img[0].shape\n",
    "    imgs_shape = (channels, height, width)\n",
    "\n",
    "    print(\"Images shape: \", imgs_shape)\n",
    "    print(\"Attributes: \", attributes)\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_num_tensor = torch.as_tensor(X_train_num.values, dtype=torch.float32)\n",
    "    X_val_num_tensor = torch.as_tensor(X_val_num.values, dtype=torch.float32)\n",
    "    X_test_num_tensor = torch.as_tensor(X_test_num.values, dtype=torch.float32)\n",
    "    X_train_img_tensor = torch.as_tensor(X_train_img, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "    X_val_img_tensor = torch.as_tensor(X_val_img, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "    X_test_img_tensor = torch.as_tensor(X_test_img, dtype=torch.float32).permute(0, 3, 1, 2)\n",
    "    y_train_tensor = torch.as_tensor(y_train.values, dtype=torch.float32).reshape(-1, 1)\n",
    "    y_val_tensor = torch.as_tensor(y_val.values, dtype=torch.float32).reshape(-1, 1)\n",
    "    y_test_tensor = torch.as_tensor(y_test.values, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset(X_train_num_tensor, X_train_img_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_num_tensor, X_val_img_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_num_tensor, X_test_img_tensor, y_test_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, attributes, imgs_shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MODEL ARCHITECTURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out), attn\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
    "                FeedForward(dim, mlp_dim, dropout = dropout)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        attentions = []  # Initialize a list to store attention weights\n",
    "        for attn, ff in self.layers:\n",
    "            attn_out, attn_weights = attn(x)  # Get both output and attention weights\n",
    "            x = attn_out + x  # Residual connection\n",
    "            attentions.append(attn_weights)  # Store attention weights\n",
    "            x = ff(x) + x  # Apply feedforward and residual connection\n",
    "\n",
    "        return self.norm(x), attentions\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x, attentions = self.transformer(x)\n",
    "        \n",
    "        # Use a dummy operation to include attention maps in the computation graph\n",
    "        dummy_contribution = attentions[-1].mean()\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        return {\"last_hidden_state\": x, \"attentions\": attentions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_divisors(n):\n",
    "    divisors = []\n",
    "    for i in range(1, int(n**0.5) + 1):\n",
    "        if n % i == 0:\n",
    "            divisors.append(i)\n",
    "            if i != n // i:  # Check to include both divisors if they are not the same\n",
    "                divisors.append(n // i)\n",
    "    divisors.sort()\n",
    "    return divisors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1(nn.Module):\n",
    "    def __init__(self, imgs_shape, patch_size):\n",
    "        super(Model1, self).__init__()\n",
    "        \n",
    "        # ViT branch\n",
    "        self.vit = ViT(\n",
    "            image_size = imgs_shape[1],\n",
    "            patch_size = patch_size,\n",
    "            dim = 32,\n",
    "            depth = 2,\n",
    "            heads = 4,\n",
    "            mlp_dim = 64,\n",
    "        )\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, vit_input):\n",
    "        vit_output = self.vit(vit_input)\n",
    "        vit_hidden_state = vit_output[\"last_hidden_state\"]\n",
    "        attentions = vit_output[\"attentions\"]  # Extract the attention weights\n",
    "        mlp_output = self.mlp(vit_hidden_state)\n",
    "        return mlp_output, attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2(nn.Module):\n",
    "    def __init__(self, imgs_shape, patch_size):\n",
    "        super(Model2, self).__init__()\n",
    "        \n",
    "        # ViT branch\n",
    "        self.vit = ViT(\n",
    "            image_size = imgs_shape[1],\n",
    "            patch_size = patch_size,\n",
    "            dim = 128,\n",
    "            depth = 4,\n",
    "            heads = 8,\n",
    "            mlp_dim = 256\n",
    "        )\n",
    "        \n",
    "        # Final MLP\n",
    "        self.final_mlp = nn.Sequential(\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, vit_input):\n",
    "        vit_output = self.vit(vit_input)\n",
    "        vit_hidden_state = vit_output[\"last_hidden_state\"]\n",
    "        attentions = vit_output[\"attentions\"]  # Extract the attention weights\n",
    "        mlp_output = self.final_mlp(vit_hidden_state)\n",
    "        return mlp_output, attentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = TINTO(problem= problem_type, blur=True, random_seed=SEED)\n",
    "#image_model = REFINED(problem= problem_type,hcIterations=5)\n",
    "#image_model = IGTD(problem= problem_type)\n",
    "#image_model = BarGraph(problem= problem_type)\n",
    "#image_model = DistanceMatrix(problem= problem_type)\n",
    "#image_model = Combination(problem= problem_type)\n",
    "#image_model = SuperTML(problem= problem_type)\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Regression/{dataset_name}/images_{dataset_name}_TINTO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iterations_per_epoch(dataset_size, batch_size):\n",
    "    iterations = dataset_size // batch_size\n",
    "    if dataset_size % batch_size != 0:\n",
    "        iterations += 1\n",
    "    return iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = calculate_iterations_per_epoch(df.shape[0], batch_size)\n",
    "# For the Boston dataset, the number of samples is too small for a range test, so the number of epochs is tripled.\n",
    "#num_epochs = num_epochs*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "645"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EXPERIMENT 2: IGTD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Get the shape of the dataframe\n",
    "num_columns = df.shape[1]\n",
    "\n",
    "# Calculate number of columns - 1\n",
    "columns_minus_one = num_columns - 1\n",
    "\n",
    "# Calculate the square root for image size\n",
    "import math\n",
    "image_size = math.ceil(math.sqrt(columns_minus_one))\n",
    "print(image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"regression\"\n",
    "image_model = IGTD(problem= problem_type, scale=[image_size,image_size], fea_dist_method='Euclidean', image_dist_method='Euclidean', error='abs', max_step=30000, val_step=300, random_seed=SEED)\n",
    "name = f\"IGTD_{image_size}x{image_size}_fEuclidean_iEuclidean_abs\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"HyNNImages/Regression/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The images are already generated\n",
      "HyNNImages/Regression/california_housing/images_california_housing_IGTD_3x3_fEuclidean_iEuclidean_abs\\regression.csv\n",
      "Images shape:  (3, 3, 3)\n",
      "Attributes:  8\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape  = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine possible patch sizes for the Vision Transformer by finding divisors of the image width\n",
    "find_divisors(imgs_shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"IGTD_3x3_fEuclidean_iEuclidean_abs_Model2_patch_s1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model_path = f\"models/Regression/{dataset_name}/ViT/{model_name}/best_model.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model2(imgs_shape, patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jiayu\\AppData\\Local\\Temp\\ipykernel_40148\\1481618732.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model2(\n",
       "  (vit): ViT(\n",
       "    (to_patch_embedding): Sequential(\n",
       "      (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=1, p2=1)\n",
       "      (1): LayerNorm((3,), eps=1e-05, elementwise_affine=True)\n",
       "      (2): Linear(in_features=3, out_features=128, bias=True)\n",
       "      (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (transformer): Transformer(\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x ModuleList(\n",
       "          (0): Attention(\n",
       "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            (attend): Softmax(dim=-1)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (to_qkv): Linear(in_features=128, out_features=1536, bias=False)\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): FeedForward(\n",
       "            (net): Sequential(\n",
       "              (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=128, out_features=256, bias=True)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Dropout(p=0.0, inplace=False)\n",
       "              (4): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (5): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (to_latent): Identity()\n",
       "  )\n",
       "  (final_mlp): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=16, out_features=1, bias=True)\n",
       "    (7): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assume `test_loader` is a PyTorch DataLoader\n",
    "# Fetch one batch of images and labels\n",
    "data_iter = iter(test_loader)\n",
    "num_data, images, labels = next(data_iter)  # Get one batch (images and labels)\n",
    "\n",
    "# Select the first image and its corresponding label (optional)\n",
    "image = images[0]  # Shape: (C, H, W)\n",
    "label = labels[0]  # (Optional, if labels are available)\n",
    "\n",
    "# Visualization (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "# Convert the tensor to a PIL image for visualization\n",
    "plt.imshow(F.to_pil_image(image))\n",
    "plt.title(f\"Label: {label.item() if labels is not None else 'No label available'}\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape for model input: torch.Size([1, 3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Ensure the image has the correct shape for the model\n",
    "image = image.unsqueeze(0).to(device)  # Add batch dimension and move to device (e.g., GPU)\n",
    "print(\"Image shape for model input:\", image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize the attention map\n",
    "def normalize_attention_map(attention_map):\n",
    "    # Apply ReLU to remove negative values\n",
    "    attention_map = np.maximum(attention_map, 0)\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    attention_map -= np.min(attention_map)\n",
    "    attention_map /= np.max(attention_map)  # Ensure max is 1\n",
    "    return attention_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAW Attention Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By extracting the attention weights from each attention head at various layers, you can visualize heatmaps over the input image patches. These maps show which regions of the image a particular head or layer attends to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [94.0..255.0].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of patches (excluding CLS token): 9, Patch size: 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#1 RAW Attention Map\n",
    "from PIL import Image  # Import Image module\n",
    "\n",
    "# Forward pass through the model\n",
    "with torch.no_grad():\n",
    "    outputs, attentions = model(image)\n",
    "\n",
    "# Function to visualize attention map with color bar\n",
    "def visualize_attention(image, attention_map, title=\"Attention Map\"):\n",
    "    # Ensure the image is in the correct format (C, H, W -> H, W, C)\n",
    "    if image.ndimension() == 4:  # If the input image has batch size\n",
    "        image = image[0]  # Use the first image in the batch\n",
    "    image = image.permute(1, 2, 0).cpu().numpy()  # Convert to HWC format (Height, Width, Channels)\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    \n",
    "    # Display the image\n",
    "    plt.imshow(image)\n",
    "    \n",
    "    # Overlay the attention map\n",
    "    im = plt.imshow(attention_map, cmap=\"jet\", alpha=0.5)  # Overlay attention with transparency\n",
    "    \n",
    "    # Add title and hide axes\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    # Add color bar for the attention map\n",
    "    plt.colorbar(im, fraction=0.046, pad=0.04)  # Show the color bar for the attention map\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# 1. Raw Attention Map\n",
    "# Extract attention weights from the last layer, average over all heads\n",
    "last_layer_attention = attentions[-1]  # Shape: (batch_size, num_heads, num_patches, num_patches)\n",
    "avg_attention = last_layer_attention.mean(dim=1)  # Average over heads\n",
    "\n",
    "# Get the shape of avg_attention to determine the number of patches\n",
    "num_patches = avg_attention.shape[-1]  # Number of patches (including CLS token)\n",
    "\n",
    "# Calculate the size of the attention map based on number of patches\n",
    "patch_size = int(np.sqrt(num_patches - 1))  # Exclude CLS token\n",
    "print(f\"Number of patches (excluding CLS token): {num_patches - 1}, Patch size: {patch_size}\")\n",
    "\n",
    "# Extract attention from CLS token to all patches and reshape to a grid\n",
    "cls_attention = avg_attention[0, 0, 1:].reshape(patch_size, patch_size).detach().cpu().numpy()  # Exclude CLS token itself\n",
    "\n",
    "# Normalize the attention map\n",
    "cls_attention_normalized = normalize_attention_map(cls_attention)\n",
    "\n",
    "# Resize attention map to match image size\n",
    "cls_attention_resized = np.array(Image.fromarray(cls_attention_normalized).resize((imgs_shape[1], imgs_shape[1]), resample=Image.BILINEAR))\n",
    "\n",
    "# Visualize Raw Attention Map\n",
    "visualize_attention(image, cls_attention_resized, title=\"Raw Attention Map (Last Layer)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since attention is compositional through layers, researchers use attention rollout techniques to aggregate attention across layers. For instance, one can multiply attention matrices across layers to track how information flows from patches to the CLS token. This can yield a single interpretable map that highlights the most influential patches for the final decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [94.0..255.0].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Attention Rollout\n",
    "# explain brieflyy\n",
    "def attention_rollout(attentions):\n",
    "    num_tokens = attentions[0].size(-1)\n",
    "    eye = torch.eye(num_tokens).unsqueeze(0).to(attentions[0].device)  # Identity matrix for residual connection\n",
    "    joint_attention = eye\n",
    "    for attention in attentions:\n",
    "        avg_attention = attention.mean(dim=1)  # Average over heads\n",
    "        joint_attention = torch.matmul(joint_attention, avg_attention + eye)  # Residual connection\n",
    "    return joint_attention\n",
    "\n",
    "rollout_attention = attention_rollout(attentions)\n",
    "cls_rollout_attention = rollout_attention[0, 0, 1:].reshape(patch_size, patch_size).detach().cpu().numpy()\n",
    "\n",
    "# Normalize the rollout attention map\n",
    "cls_rollout_attention_normalized = normalize_attention_map(cls_rollout_attention)\n",
    "\n",
    "# Resize rollout attention map to match image size\n",
    "rollout_attention_resized = np.array(Image.fromarray(cls_rollout_attention_normalized).resize((imgs_shape[1], imgs_shape[1]), resample=Image.BILINEAR))\n",
    "\n",
    "# Visualize Attention Rollout Map\n",
    "visualize_attention(image, rollout_attention_resized, title=\"Attention Rollout Map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient based - LeGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure input tensor supports gradients\n",
    "image.requires_grad_()\n",
    "\n",
    "# Forward pass to get output and attentions\n",
    "outputs, attentions = model(image)\n",
    "\n",
    "# Assuming regression task, extract scalar output\n",
    "output_scalar = outputs.squeeze()  # Single scalar value for batch size = 1\n",
    "regression_value = output_scalar.item()\n",
    "\n",
    "# Extract attention map from the last layer\n",
    "last_attention_map = attentions[-1]  # Shape: (Batch, Heads, Tokens, Tokens)\n",
    "\n",
    "# Zero gradients in the model\n",
    "model.zero_grad()\n",
    "\n",
    "# Backward pass to compute gradients of the regression output\n",
    "output_scalar.backward()\n",
    "\n",
    "# Now, last_attention_map.grad contains the gradients\n",
    "gradients = last_attention_map.grad  # Gradients w.r.t. attention map\n",
    "\n",
    "# Normalize and visualize the gradients as a heatmap\n",
    "# Use the mean of the gradients across the heads\n",
    "pooled_gradients = torch.mean(gradients, dim=1)  # Mean over heads\n",
    "attention_gradient_map = pooled_gradients.squeeze()  # Shape: (Tokens, Tokens)\n",
    "\n",
    "# Reshape or resize the gradient map if needed for visualization\n",
    "# ViT token attention maps may need to be converted back to image space\n",
    "def normalize_attention_map(attention_map):\n",
    "    attention_map -= attention_map.min()\n",
    "    attention_map /= attention_map.max()\n",
    "    return attention_map\n",
    "\n",
    "# Normalize and visualize\n",
    "attention_gradient_map_normalized = normalize_attention_map(attention_gradient_map.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
